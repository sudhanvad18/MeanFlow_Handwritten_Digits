# MeanFlow-MNIST (Optimized)

ğŸš€ An optimized PyTorch implementation of **MeanFlow generative modeling** applied to **MNIST digits**.  
This project combines flow-matching training objectives with modern architectural and optimization improvements to achieve **stable training, high-quality samples, and efficient inference**.

---

## ğŸ”‘ Key Features

- **Flow Matching Training** ([Lipman et al., 2023](https://arxiv.org/abs/2210.02747)):  
  Learns a neural ODE that directly matches the true data velocity field.
- **Fourier Time Embeddings**:  
  Encodes scalar time `t` into high-dimensional features with sinusoidal frequencies for stronger conditioning.
- **UNet Backbone with Residual Blocks**:  
  Lightweight but expressive architecture tailored for MNIST (28Ã—28), with skip connections and residual refinement.
- **Average Velocity Loss**:  
  Adds stability by matching the velocity at `t=0` to a Monte Carlo average over intermediate times.
- **Heunâ€™s Method Sampler**:  
  A second-order ODE solver for better numerical stability and sharper generated samples.
- **Exponential Moving Average (EMA) Weights**:  
  Ensures smoother convergence and higher-quality samples at inference.
- **Cosine Learning Rate Scheduler + Checkpointing**:  
  Efficient training with automatic restarts and state saving (including EMA state).

---

## ğŸ“‚ Project Structure

â”œâ”€â”€ meanflow_mnist_optimized.py # Main training & sampling script
â”œâ”€â”€ samples/ # Generated MNIST samples during training
â”œâ”€â”€ checkpoints/ # Saved checkpoints (model + optimizer + EMA)
â”œâ”€â”€ data/ # MNIST dataset (downloaded automatically)
â””â”€â”€ README.md # Project documentation


---

## âš™ï¸ Requirements

- Python 3.9+
- PyTorch â‰¥ 2.0
- torchvision
- CUDA GPU recommended (for speed)

Install dependencies:

```bash
pip install torch torchvision

--- 
## ğŸš€ Training

Run the main script:
```
python meanflow_mnist_optimized.py
Training will:

Save checkpoints every 1000 steps (default).

Generate sample grids to samples/.

Save the final sample grid to outputs/mnist_digits_final.png.

---

## ğŸ“Š Model Overview

### Fourier Time Embedding
Encodes time *t* into a feature vector:

\[
\phi(t) = \text{MLP}\big[ \sin(\omega t), \cos(\omega t) \big]
\]

with logarithmically spaced frequencies.

---

### UNet + Residual Blocks
- **Downsampling path**: convolutional feature extraction.  
- **Bottleneck**: residual refinement with time conditioning.  
- **Upsampling path**: skip connections and reconstruction.  

---

### Loss Function
The training objective is a combination:

\[
\mathcal{L} = 
\underbrace{\| v_\theta(z_t, t) - (x - z_t) \|^2}_{\text{Flow Matching}}
+ \lambda \,
\underbrace{\| v_\theta(z_0, 0) - \bar{v} \|^2}_{\text{Average Velocity}}
\]

## ğŸ“ˆ Results

The model generates MNIST-like handwritten digits using the MeanFlow approach.  
After sufficient training (â‰ˆ50k steps), digits become increasingly clear and structured.

### Qualitative Samples
Generated images show progressive improvement in digit quality as training advances:

- **Early training (~10k steps):** blurry, noisy shapes without clear digit structure.  
- **Mid training (~30k steps):** digits start forming but remain fuzzy.  
- **Later training (~50k steps):** sharp digits resembling MNIST samples, though with occasional artifacts.  

<p align="center">
  <img src="samples/mnist_digits.png" alt="Generated MNIST Digits" width="400">
</p>

### Key Observations
- Increasing the number of sampling steps (e.g., 50 â†’ 100) improves sharpness.  
- EMA (Exponential Moving Average) weights further stabilize outputs.  
- Model capacity (base channels) directly influences clarity vs. memory usage.  

---
## ğŸ“š References & Future Work

### References
This work builds on the foundations of **flow matching** and **MeanFlow** models:

- Lipman, Yaron, et al. *"Flow Matching for Generative Modeling."* (NeurIPS 2023)  
- Albergo, Michael S., et al. *"Building normalizing flows with stochastic interpolants."* (ICML 2023)  
- Official MeanFlow GitHub: [haidog-yaqub/MeanFlow](https://github.com/haidog-yaqub/MeanFlow)  

The training code and architecture here are adapted for the **MNIST** dataset, with practical modifications:
- Fourier time embeddings for richer temporal encoding.  
- UNet backbone with residual connections for improved capacity.  
- Heunâ€™s method for more stable ODE integration.  
- Combined **flow matching** + **average-velocity** losses.  

---

### ğŸš€ Future Work
There are several directions to push this project further:

1. **Higher-Resolution Datasets**  
   Extend beyond MNIST (28Ã—28) to datasets like **CIFAR-10** (32Ã—32) or **CelebA** (64Ã—64+).  
   This would test scalability and generalization.

2. **Stronger Architectures**  
   - Replace UNetSmall with a full UNet backbone or Transformer blocks.  
   - Explore attention mechanisms for structured generation.  

3. **Advanced Samplers**  
   - Investigate adaptive ODE solvers (e.g., RK45) instead of fixed Heun/Euler steps.  
   - Incorporate variance reduction for faster sampling.  

4. **Hybrid Training Objectives**  
   - Blend MeanFlow with diffusion-style noise prediction.  
   - Explore score distillation loss variants.  

5. **Evaluation Metrics**  
   - Add FID (FrÃ©chet Inception Distance) or IS (Inception Score) to benchmark results.  
   - Compare with diffusion baselines on MNIST and CIFAR.  

---

ğŸ“Œ *This project currently demonstrates the viability of MeanFlow on MNIST. Future expansions aim to bridge the gap between academic research and practical large-scale generative modeling.*  
